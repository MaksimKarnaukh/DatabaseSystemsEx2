# Database systems: Exercise 2

## Group members

Thomas Gueutal, Bas Tobback, Billy Vanhove en Maksim Karnaukh

## Challenge: Implement one of three algorithms!

This time, we used python for our implementation and no longer jupyter notebooks. We decided to 
implement the extendible hashing algorithm where a limited number of buckets fits into memory.
All the code can be found in extendible_hashing.py

### Implementation (classes)

We decided to use three different datastructures. First we have the Bucket class. This class
stores the bucketID which is later used to retrieve buckets by id from 'disk'. it also stores the
size of the prefix in the bucket, which increases with every split that occurs with this bucket. Each
bucket has a max size, which represents the max number of BucketValues that may be stored in 1 bucket. At
last, it stores the actual list of BucketValues. This is our second class. an instance of BucketValue holds
a key and a value. The key refers to the hashed key that is used to store elements in a bucket. The
value is the value that belongs to this key, which can be found on disk, or occasionally, in memory.
Finally, we have the ExtendibleHashingIndex class which holds everything related to the algorithm.
it has a dictionary of prefixes as keys and pointers to buckets as values. it also holds the length of
the largest prefix and the max number of buckets that may be in memory at some point in time. 

### Implementation (methods)

Most of the functionality happens in the ExtendibleHashingIndex class. This is where you insert keys
with their values using the insert_keyval(k,v) method. This will request the bucket in which the insert
needs to happen. Then, the insert method will be called on this bucket, which returns a boolean. If False,
it means the bucket is full. In that case, the split method will be called 1 or more times to split the 
bucket into 2 new buckets. After each split, the keys from the old bucket will be redistributed over the
new buckets. This will keep happening until there is a bucket that has space to store the key we want to 
insert. \
\
What we mean with 'this will keep happening' is that if the max bucket size is 2 and there is a bucket
with values: 11010110... and 11010100... and the current prefix is 11, then if it gets split into buckets
110 and 111, both values of the old bucket will still be in the first bucket because both start with 110.
If you then need to insert the value 1101000... then you will need to split twice more before there is a
new bucket that has enough space to hold the new key. \
\
At the end, the insertion should be successful. At first, we tested the algorithm without being concerned
about a difference between memory and disk (means we just loaded all buckets in memory to test the
algorithm itself). We randomised a list of 10000 user_ids as key and value and inserted them in the
random order that was generated by the random.shuffle() method, provided by pythons random library.
The reason we did this, is because we feel like if 10000 insertions happen in a random order, and it 
ends in a valid state every time, the algorithm is likely to work because a failure in one of the 10000
insertions will mean that the algorithm ends in an invalid state. \
\
To check this, we created 2 functions: isValid() and getViolations(). The isValid() function will
return True if the ExtendibleHashingIndex is in a valid state at some point in time (you can call
this function whenever you want). If it returns False, you know that something is wrote but you don't know
what exactly is wrong. That's where the getViolations() function comes into play. If you print the output
of this function, all the violations in the ExtendibleHashingIndex will be printed. \
Those functions are used purely for debugging reasons, but also allow us to be sure that future updates
to our code didn't contain any errors, such as when we implemented the disk storage.

### Implementation (disk storage vs memory)

Our implementation only allows to have a certain number of buckets in memory at any point in time. Because
of that, we needed to make sure to write buckets to a file. When we do a getBucket() operation, we will
either get the bucket directly from memory or, if the bucket isn't present in memory, load it from disk and
replace a bucket that is in memory. Another challenge was when we did the split operation. Say x buckets
may be present in memory. If all x places are filled and you then do a split, then 1 bucket needed to have 
its changes written to disk, to make place for the 2nd bucket that is created in the split. \
\
However, a challenge we faced was that the buckets to which the pointers to buckets point, could not all be 
saved in memory. We resolved this by having a wrapper called BucketWrapper, which holds a Union object with
a Bucket and integer as attribute. The integer is the keyhash (which can be saved in memory) and the Bucket
is the Bucket corresponding to the keyhash. So, the bucket pointers point to BucketWrapper objects now. If
the content of the Union is an integer, you can conclude that the bucket isn't in memory so the first operation
is to load it to memory. If the Union is a Bucket, then the bucket is already present in memory, so we don't 
need to do additional operations. \
\
The way the data is stored 'on disk' is by assuming that each bucket has a fixed size. That is, we reserve a fixed size
block of memory on-file for every bucket. The block contains space for: The local prefix size member, the maximum length
of the BucketValue list, the current length of the BucketValue list and space to store the maximum amount of BucketValues.
So if the max length of the BucketValue list is 10, but it currently contains 4 items, then the block still contains
space for all 10 potential items. With this assumption,  we can easily calculate the offset of the bucket in the binary
file, and we wouldn't have to parse the entire file each time.

We are only able to assume that every bucket requires the same fixed amount of space, 207 bytes in out implementation,
because we assume the BucketValue's key and value members always have a size of respectively 4B and 16B on disk. These
numbers were calculated as follows:

* key: our database in db.py makes use of a fixed size user ID value for every variable length tuple, namely 4B. The string representation of the keys in the index will thus contain 32 "binary characters" and take 4B in the index's storage file
* value: our database in db.py makes use of a fixed size tuple address of 16B

The use of these "fixed" values is more or less generalized in our index implementation. By editing the environment
variables at the top of the [extendible_hashing.py](extendible_hashing.py) file, the changed sizes should be taken
into account by the index. But a mismatch between the [db.py](db.py) database and the index will be created if only
the environment variables are changed.

