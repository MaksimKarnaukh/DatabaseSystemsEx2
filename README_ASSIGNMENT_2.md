# Database systems: Exercise 2

## Group members

Thomas Gueutal, Bas Tobback, Billy Vanhove en Maksim Karnaukh

## Challenge: Implement one of three algorithms!

This time, we used python for our implementation and no longer jupyter notebooks. We decided to 
implement the extendible hashing algorithm where a limited number of buckets fits into memory.
All the code can be found in extendible_hashing.py

### Implementation (classes)

We decided to use three different datastructures. First we have the Bucket class. This class
stores the bucketID which is later used to retrieve buckets by id from 'disk'. it also stores the
size of the prefix in the bucket, which increases with every split that occurs with this bucket. Each
bucket has a max size, which represents the max number of BucketValues that may be stored in 1 bucket. At
last, it stores the actual list of BucketValues. This is our second class. an instance of BucketValue holds
a key and a value. The key refers to the hashed key that is used to store elements in a bucket. The
value is the value that belongs to this key, which can be found on disk, or occasionally, in memory.
Finally, we have the ExtendibleHashingIndex class which holds everything related to the algorithm.
it has a dictionary of prefixes as keys and pointers to buckets as values. it also holds the length of
the largest prefix and the max number of buckets that may be in memory at some point in time. 

### Implementation (methods)

Most of the functionality happens in the ExtendibleHashingIndex class. This is where you insert keys
with their values using the insert_keyval(k,v) method. This will request the bucket in which the insert
needs to happen. Then, the insert method will be called on this bucket, which returns a boolean. If False,
it means the bucket is full. In that case, the split method will be called 1 or more times to split the 
bucket into 2 new buckets. After each split, the keys from the old bucket will be redistributed over the
new buckets. This will keep happening until there is a bucket that has space to store the key we want to 
insert. \
\
What we mean with 'this will keep happening' is that if the max bucket size is 2 and there is a bucket
with values: 11010110... and 11010100... and the current prefix is 11, then if it gets split into buckets
110 and 111, both values of the old bucket will still be in the first bucket because both start with 110.
If you then need to insert the value 1101000... then you will need to split twice more before there is a
new bucket that has enough space to hold the new key. \
\
At the end, the insertion should be successful. At first, we tested the algorithm without being concerned
about a difference between memory and disk (means we just loaded all buckets in memory to test the
algorithm itself). We randomised a list of 10000 user_ids as key and value and inserted them in the
random order that was generated by the random.shuffle() method, provided by pythons random library.
The reason we did this, is because we feel like if 10000 insertions happen in a random order, and it 
ends in a valid state every time, the algorithm is likely to work because a failure in one of the 10000
insertions will mean that the algorithm ends in an invalid state. \
\
To check this, we created 2 functions: isValid() and getViolations(). The isValid() function will
return True if the ExtendibleHashingIndex is in a valid state at some point in time (you can call
this function whenever you want). If it returns False, you know that something is wrote but you don't know
what exactly is wrong. That's where the getViolations() function comes into play. If you print the output
of this function, all the violations in the ExtendibleHashingIndex will be printed. \
Those functions are used purely for debugging reasons, but also allow us to be sure that future updates
to our code didn't contain any errors, such as when we implemented the disk storage.

### Implementation (disk storage vs memory)

WIP
